{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.core.display import display, HTML\n",
    "from bs4 import BeautifulSoup, Comment, Tag, NavigableString\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected-metadata.json expected.html          source.html\r\n"
     ]
    }
   ],
   "source": [
    "!ls 'nytimes-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 6), match='banner'>\n",
      "<_sre.SRE_Match object; span=(0, 11), match='Breadcrumbs'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(re.search(REGEXPS['unlikelyCandidates'], 'banner'))\n",
    "print(re.search(REGEXPS['unlikelyCandidates'], 'Breadcrumbs'))\n",
    "print(not re.search(REGEXPS['okMaybeItsACandidate'], 'Breadcrumbshadow'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEXPS = {\n",
    "    \"unlikelyCandidates\": re.compile(r\"hidden|banner|breadcrumbs|combx|comment|community|cover-wrap|disqus|extra|foot|header|legends|menu|related|remark|replies|rss|shoutbox|sidebar|skyscraper|social|sponsor|supplemental|ad-break|agegate|pagination|pager|popup|yom-remote|ad\", re.I),\n",
    "    \"okMaybeItsACandidate\": re.compile(r\"and|article|body|column|main|shadow\", re.I),\n",
    "    \"positive\": re.compile(r\"article|body|content|entry|hentry|h-entry|main|page|pagination|post|text|blog|story\", re.I),\n",
    "    \"negative\": re.compile(r\"hidden|^hid$| hid$| hid |^hid |banner|combx|comment|com-|contact|foot|footer|footnote|masthead|media|meta|outbrain|promo|related|scroll|share|shoutbox|sidebar|skyscraper|sponsor|shopping|tags|tool|widget\", re.I),\n",
    "    \"extraneous\": re.compile(r\"print|archive|comment|discuss|e[\\-]?mail|share|reply|all|login|sign|single|utility\", re.I),\n",
    "    \"byline\": re.compile(r\"byline|author|dateline|writtenby|p-author\", re.I),\n",
    "    #\"replaceFonts\": re.compile(r\"<(\\/?)font[^>]*>\", re.I),\n",
    "    \"normalize\": re.compile(r\"\\s{2,}\"),\n",
    "    \"videos\": re.compile(r\"\\/\\/(www\\.)?(dailymotion|youtube|youtube-nocookie|player\\.vimeo)\\.com\", re.I),\n",
    "    \"nextLink\": re.compile(r\"(next|weiter|continue|>([^\\|]|$)|»([^\\|]|$))\", re.I),\n",
    "    \"prevLink\": re.compile(r\"(prev|earl|old|new|<|«)\", re.I),\n",
    "    \"whitespace\": re.compile(r\"^\\s*/\"),\n",
    "    \"hasContent\": re.compile(r\"\\S$/\"),\n",
    "    \"hasTextContent\": re.compile(r\"\\S\"),\n",
    "}\n",
    "\n",
    "DEFAULT_TAGS_TO_SCORE = [\"section\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"td\", \"pre\"]\n",
    "NUM_OF_TOP_CANDIDATES = 5\n",
    "\n",
    "def remove_scripts(doc):\n",
    "    nodes = doc.find_all('script')\n",
    "    for n in nodes: n.extract()\n",
    "        \n",
    "    no_scripts = doc.find_all('noscript')\n",
    "    for n in no_scripts: n.extract()\n",
    "    \n",
    "    head = doc.find('head')\n",
    "    for el in head(text=lambda text: isinstance(text, Comment)): \n",
    "        el.extract()\n",
    "        \n",
    "    return doc\n",
    "\n",
    "\"\"\"\n",
    "Replaces 2 or more successive <br> elements with a single <p>.\n",
    "\"\"\"\n",
    "def replace_brs(elem, doc):\n",
    "    all_brs = elem.find_all('br')\n",
    "    \n",
    "    for br in all_brs: \n",
    "        if not br: continue\n",
    "        nxt = br.next_element\n",
    "        replaced = False\n",
    "        \n",
    "        while nxt != None and is_tag(nxt.next_element) and nxt.next_element.name == 'br':\n",
    "            replaced = True\n",
    "            nxt = nxt.next_element\n",
    "            brSibl = nxt.next_element\n",
    "            nxt.extract()\n",
    "            nxt = brSibl\n",
    "        \n",
    "        if replaced:\n",
    "            p = doc.new_tag('p')\n",
    "            br.replace_with(p)\n",
    "            \n",
    "            if p.next_element != None:\n",
    "                nxt = p.next_element\n",
    "                while nxt != None:\n",
    "                    if type(nxt) == Tag and \\\n",
    "                    nxt.name == 'br' and \\\n",
    "                    nxt.next_element != None and \\\n",
    "                    type(nxt.next_element) == Tag and \\\n",
    "                    nxt.next_element.name == 'br':\n",
    "                        break;\n",
    "\n",
    "                    sibl = nxt.next_element\n",
    "                    if sibl:\n",
    "                        p.append(sibl)\n",
    "                    nxt = sibl\n",
    "        \n",
    "    return doc\n",
    "\n",
    "def prep_document(doc):\n",
    "    styles = doc.find_all('style')\n",
    "    for n in styles: n.extract()\n",
    "        \n",
    "    body = doc.body\n",
    "    doc = replace_brs(body, doc)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def check_byline(node, match_string):\n",
    "    pass\n",
    "\n",
    "def get_article(doc):\n",
    "    FLAG_STRIP_UNLIKELYS = True\n",
    "    \n",
    "    page = doc.body\n",
    "    if not page:\n",
    "        return None\n",
    "    \n",
    "    page_cache = inner_html(page)\n",
    "    \n",
    "#     while True:\n",
    "    stripUnlinkelyCandidates = FLAG_STRIP_UNLIKELYS\n",
    "    elementsToScore = []\n",
    "    node = page\n",
    "\n",
    "    while node != None:\n",
    "        matchString = get_class_name(node) + \" \" + get_id_str(node) + \" \" + get_role_attr(node)\n",
    "        # TODO: checkByline(node)\n",
    "\n",
    "        # Remove Unlikely Candidates\n",
    "        if stripUnlinkelyCandidates:\n",
    "            if re.search(REGEXPS['unlikelyCandidates'], matchString) \\\n",
    "            and not re.search(REGEXPS['okMaybeItsACandidate'], matchString) \\\n",
    "            and node.name != 'body' and node.name != 'a':\n",
    "                # print('debug: Removing unlikely candidate - ', matchString)\n",
    "                node = remove_and_get_next(node)\n",
    "                continue\n",
    "\n",
    "        #Remove DIV, SECTION, and HEADER nodes without any content(e.g. text, image, video, or iframe).\n",
    "        if is_empty_candidate(node) and is_element_without_content(node):\n",
    "            node = remove_and_get_next(node)\n",
    "            continue\n",
    "\n",
    "        if node.name in DEFAULT_TAGS_TO_SCORE:\n",
    "            elementsToScore.append(node)\n",
    "\n",
    "        if node.name == 'div':\n",
    "            if has_single_p_inside_element(node):\n",
    "                new_node = get_children(node)[0]\n",
    "                node.replace_with(new_node)\n",
    "                node = new_node\n",
    "                elementsToScore.append(node)\n",
    "                \n",
    "            if has_child_block_elements(node):\n",
    "                node.name = 'p'\n",
    "                elementsToScore.append(node)\n",
    "        \n",
    "        node = get_next_node(node)\n",
    "\n",
    "    candidates = []\n",
    "    for elToScore in elementsToScore:\n",
    "        if elToScore.parent == None:\n",
    "            continue\n",
    "                \n",
    "        innerText = elToScore.get_text().strip()\n",
    "        if len(innerText) < 25:\n",
    "            continue\n",
    "        \n",
    "        ancestors = get_ancestors(elToScore, 3)\n",
    "        if len(ancestors) == 0:\n",
    "            continue\n",
    "        \n",
    "        content_score = 0\n",
    "        \n",
    "        # add a point for the paragraph itself as a base\n",
    "        content_score += 1\n",
    "        \n",
    "        # add points for any commas in the paragraph\n",
    "        content_score += len(innerText.split(','))\n",
    "        \n",
    "        content_score += min(len(innerText)//100, 3)\n",
    "        \n",
    "        for level, ances in enumerate(ancestors):\n",
    "            if ances.name == '' or ances.name == '[document]':\n",
    "                continue\n",
    "            \n",
    "            if not was_score_initialized(ances):\n",
    "                initialize_node(ances)\n",
    "                candidates.append(ances)\n",
    "            \n",
    "            if level == 0:\n",
    "                score_divider = 1\n",
    "            elif level == 1:\n",
    "                score_divider = 2\n",
    "            else:\n",
    "                score_divider = level * 3\n",
    "                \n",
    "            set_score(ances, get_score(ances) + get_score(ances) // score_divider)\n",
    "        \n",
    "    top_candidates = []\n",
    "    for c in top_candidates:\n",
    "        adjusted_score = int(get_score(c) * (1 - get_link_density()))\n",
    "        set_score(c, adjusted_score)\n",
    "    \n",
    "    top_candidates = sorted(candidates, key=lambda x: get_score(x), reverse=True)\n",
    "    top_candidates = top_candidates[:NUM_OF_TOP_CANDIDATES]\n",
    "    \n",
    "    if len(top_candidates) == 0:\n",
    "        return doc.body.get_text()\n",
    "    \n",
    "    txt = [re.sub(REGEXPS['normalize'], \" \", n.get_text()) for n in top_candidates]\n",
    "    # debug, see simplified cleaned html\n",
    "    #txt = [n for n in top_candidates]\n",
    "    return txt[0]\n",
    "\n",
    "\n",
    "def parse(file_path):\n",
    "    fp = open(file_path)\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "    doc = copy.copy(soup)\n",
    "    \n",
    "    doc = remove_scripts(soup) \n",
    "    doc = prep_document(doc)\n",
    "    content = get_article(doc)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " United Nations peacekeepers at a refugee camp in Sudan on Monday. In exchange for the lifting of United States trade sanctions, Sudan has said it will improve access for aid groups, stop supporting rebels in neighboring South Sudan and cooperate with American intelligence agents. Ashraf Shazly/Agence France-Presse — Getty Images LONDON — After nearly 20 years of hostile relations, the American government plans to reverse its position on Sudan and lift trade sanctions, Obama administration officials said late Thursday.\n",
      "Sudan is one of the poorest, most isolated and most violent countries in Africa, and for years the United States has imposed punitive measures against it in a largely unsuccessful attempt to get the Sudanese government to stop killing its own people.\n",
      "On Friday, the Obama administration will announce a new Sudan strategy. For the first time since the 1990s, the nation will be able to trade extensively with the United States, allowing it to buy goods like tractors and spare parts and attract much-needed investment in its collapsing economy.\n",
      "In return, Sudan will improve access for aid groups, stop supporting rebels in neighboring South Sudan, cease the bombing of insurgent territory and cooperate with American intelligence agents.\n",
      "American officials said Sudan had already shown important progress on a number of these fronts. But to make sure the progress continues, the executive order that President Obama plans to sign on Friday, days before leaving office, will have a six-month review period. If Sudan fails to live up to its commitments, the embargo can be reinstated. Analysts said good relations with Sudan could strengthen moderate voices within the country and give the Sudanese government incentives to refrain from the brutal tactics that have defined it for decades.\n",
      "In 1997, President Bill Clinton imposed a comprehensive trade embargo against Sudan and blocked the assets of the Sudanese government, which was suspected of sponsoring international terrorism. In the mid-1990s, Osama bin Laden lived in Khartoum, the capital, as a guest of Sudan’s government.\n",
      "In 1998, Bin Laden’s agents blew up the United States Embassies in Kenya and Tanzania, killing more than 200 people. In retaliation, Mr. Clinton ordered a cruise missile strike against a pharmaceutical factory in Khartoum.\n",
      "Since then, American-Sudanese relations have steadily soured. The conflict in Darfur, a vast desert region of western Sudan, was a low point. After rebels in Darfur staged an uprising in 2003, Sudanese security services and their militia allies slaughtered tens of thousands of civilians, leading to condemnation around the world, genocide charges at the International Criminal Court against Sudan’s president, Omar Hassan al-Bashir, and a new round of American sanctions.\n",
      "American officials said Thursday that the American demand that Mr. Bashir be held accountable had not changed. Neither has Sudan’s status as one of the few countries, along with Iran and Syria, that remain on the American government’s list of state sponsors of terrorism.\n",
      "Sales of military equipment will still be prohibited, and some Sudanese militia and rebel leaders will still face sanctions.\n",
      "But the Obama administration is clearly trying to open a door to Sudan. There is intense discontent across the country, and its economy is imploding. American officials have argued for years that it was time to help Sudan dig itself out of the hole it had created.\n",
      "Officials divulged Thursday that the Sudanese government had allowed two visits by American operatives to a restricted border area near Libya, which they cited as evidence of a new spirit of cooperation on counterterrorism efforts. In addition to continuing violence in Darfur, several other serious conflicts are raging in southern and central Sudan, along with a civil war in newly independent South Sudan, which Sudan has been suspected of inflaming with covert arms shipments.\n",
      "Eric Reeves, one of the leading American academic voices on Sudan, said he was “appalled” that the American government was lifting sanctions.\n",
      "He said that Sudan’s military-dominated government continued to commit grave human rights abuses and atrocities, and he noted that just last week Sudanese security services killed more than 10 civilians in Darfur.\n",
      "“There is no reason to believe the guys in charge have changed their stripes,” said Mr. Reeves, a senior fellow at the François-Xavier Bagnoud Center for Health and Human Rights at Harvard University. “These guys are the worst of the worst.”\n",
      "Obama administration officials said that they had briefed President-elect Donald J. Trump’s transition team, but that they did not know if Mr. Trump would stick with a policy of warmer relations with Sudan.\n",
      "They said that Sudan had a long way to go in terms of respecting human rights, but that better relations could help increase American leverage.\n",
      "Mr. Reeves said he thought that the American government was being manipulated and that the Obama administration had made a “deal with the devil.” Continue reading the main story\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_file = 'nytimes-1/source.html'\n",
    "\n",
    "d = parse(test_file)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "\n",
    "def inner_html(node):\n",
    "    return node.encode_contents()\n",
    "\n",
    "def get_children(node):\n",
    "    if node == None:\n",
    "        return []\n",
    "    return list(filter(lambda x: is_tag(x), list(node.children)))\n",
    "\n",
    "def get_ancestors(node, maxDepth = 0):\n",
    "    i = 0\n",
    "    ancestors = []\n",
    "    while node != None and node.parent != None:\n",
    "        ancestors.append(node.parent)\n",
    "        i += 1\n",
    "        if maxDepth and i == maxDepth:\n",
    "            break\n",
    "        node = node.parent\n",
    "    return ancestors\n",
    "\n",
    "def has_single_p_inside_element(element):\n",
    "    ch = get_children(element)\n",
    "    if len(ch) != 1 or ch[0].name != 'p':\n",
    "        return False\n",
    "    has_content = [c for c in element.contents \\\n",
    "             if type(c) == NavigableString and re.search(REGEXPS['hasTextContent'], c) != None]\n",
    "    return not any(has_content)\n",
    "    \n",
    "def has_child_block_elements(element):\n",
    "    DIV_TO_P_ELEMS = [\"a\", \"blockquote\", \"dl\", \"div\", \"img\", \"ol\", \"p\", \"pre\", \"table\", \"ul\", \"select\" ]\n",
    "    hasBlock = False\n",
    "    for node in get_children(element):\n",
    "        hasBlock = node.name in DIV_TO_P_ELEMS or has_child_block_elements(node)\n",
    "    return hasBlock\n",
    "    \n",
    "def is_empty_candidate(node):\n",
    "    return node.name == 'div' or node.name == 'section' or node.name == 'header' \\\n",
    "            or node.name == 'h1' or node.name == 'h2' or node.name == 'h3' \\\n",
    "            or node.name == 'h4' or node.name == 'h5' or node.name == 'h6'\n",
    "\n",
    "def is_element_without_content(node):\n",
    "    return len(node.get_text().strip()) == 0 and (len(node.contents) == 0 or \\\n",
    "                                                 len(node.contents) == len(node.find_all('br')) + len(node.find_all('hr')))\n",
    "\n",
    "def remove_and_get_next(node):\n",
    "    nxt_node = get_next_node(node, True)\n",
    "    node.extract()\n",
    "    return nxt_node\n",
    "\n",
    "def get_next_node(node, ignoreSelfAndKids=False):\n",
    "    if(not ignoreSelfAndKids and first_element_child(node)):\n",
    "        return first_element_child(node)\n",
    "    \n",
    "    if next_element_sibling(node):\n",
    "        return next_element_sibling(node)\n",
    "    \n",
    "    node = node.parent\n",
    "    while node and not next_element_sibling(node):\n",
    "        node = node.parent\n",
    "    \n",
    "    return node and next_element_sibling(node)\n",
    "\n",
    "def get_class_name(node):\n",
    "    if 'class' not in node.attrs:\n",
    "        return \"\"\n",
    "    return str.join(' ', node.get('class'))\n",
    "\n",
    "def get_id_str(node):\n",
    "    if not node.get('id'):\n",
    "        return \"\"\n",
    "    return node.get('id')\n",
    "\n",
    "def get_role_attr(node):\n",
    "    if not node.get('role'):\n",
    "        return \"\"\n",
    "    return node.get('role')\n",
    "\n",
    "def is_tag(item):\n",
    "    return item != None and type(item) == Tag\n",
    "\n",
    "def first_element_child(n):\n",
    "    el = False\n",
    "    for c in n.children:\n",
    "        if not el and is_tag(c):\n",
    "            el = c\n",
    "            break\n",
    "    return el\n",
    "\n",
    "def next_element_sibling(n):\n",
    "    el = False\n",
    "    for c in n.next_siblings:\n",
    "        if not el and is_tag(c):\n",
    "            el = c\n",
    "            break\n",
    "    return el\n",
    "\n",
    "\n",
    "# Get the density of links as a percentage of the content\n",
    "# This is the amount of text that is inside a link divided by the total text in the node.\n",
    "def get_link_density(element):\n",
    "    txt_len = len(element.get_text())\n",
    "    if txt_len == 0:\n",
    "        return 0\n",
    "    \n",
    "    link_len = 0\n",
    "    links = element.find_all('a')\n",
    "    for l in links:\n",
    "        link_len += len(l.get_text())\n",
    "        \n",
    "    return link_len / txt_len\n",
    "\n",
    "# Content score helpers\n",
    "def was_score_initialized(node):\n",
    "    return node.get('data-contentscore') != None\n",
    "\n",
    "def get_score(node):\n",
    "    if not node.get('data-contentscore'):\n",
    "        return 0\n",
    "    return int(node['data-contentscore'])\n",
    "\n",
    "def set_score(node, score):\n",
    "    if type(score) == 'str':\n",
    "        score = int(score)\n",
    "    node['data-contentscore'] = str(score)\n",
    "\n",
    "def get_class_weight(node):\n",
    "    # TODO: Add flag conditional, might not be needed after first pass\n",
    "    weight = 0\n",
    "    \n",
    "    className = get_class_name(node)\n",
    "    if className.strip() != \"\":\n",
    "        if re.search(REGEXPS['negative'], className) != None:\n",
    "            weight -= 25\n",
    "        if re.search(REGEXPS['positive'], className) != None:\n",
    "            weight += 25\n",
    "            \n",
    "    idstr = get_id_str(node)\n",
    "    if idstr != \"\":\n",
    "        if re.search(REGEXPS['negative'], idstr) != None:\n",
    "            weight -= 25\n",
    "        if re.search(REGEXPS['positive'], idstr) != None:\n",
    "            weight += 25\n",
    "            \n",
    "    return weight\n",
    "\n",
    "    \n",
    "def initialize_node(node):\n",
    "    set_score(node, 0)\n",
    "    if node.name == 'div':\n",
    "        set_score(node, get_score(node) + 5)\n",
    "    elif node.name in ['pre', 'td', 'blockquote']:\n",
    "        set_score(node, get_score(node) + 3)\n",
    "    elif node.name in ['address','ol','ul','dl','dd','dt','li','form']:\n",
    "        set_score(node, get_score(node) - 3)\n",
    "    elif node.name in ['h1','h2','h3','h4','h5','h6','th']:\n",
    "        set_score(node, get_score(node) - 5)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    set_score(node, get_score(node) + get_class_weight(node))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "test_doc = \"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "    <head></head>\n",
    "    <body>\n",
    "        <h1>My First Heading</h1>\n",
    "        <div>\n",
    "            <p>Test paragraph</p>\n",
    "            <a>Testing</a>\n",
    "            <hr />\n",
    "        </div>\n",
    "        <div>\n",
    "             lost text\n",
    "              <p>Single p content</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <p>good to go</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <blockquote>some quote</blockquote>\n",
    "        </div>\n",
    "        <div><br><br></div>\n",
    "        <div>i</div>\n",
    "        another \n",
    "        <p>My first paragraph.</p>\n",
    "    </body>\n",
    "\n",
    "</html>\"\"\"\n",
    "\n",
    "sp = BeautifulSoup(test_doc, \"lxml\")\n",
    "\n",
    "mstr = sp.find('h1')\n",
    "first_child = first_element_child(mstr)\n",
    "\n",
    "print(len(get_ancestors(mstr)))\n",
    "\n",
    "# n = first_child\n",
    "# tosc = []\n",
    "# while n != None:\n",
    "#     print(n, has_child_block_elements(n))\n",
    "# #     is_element_without_content(n)\n",
    "# #     if n.name in DEFAULT_TAGS_TO_SCORE:\n",
    "# #         tosc.append(n)\n",
    "#     n = get_next_node(n)\n",
    "#     print()\n",
    "# print(tosc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_br = '<body id=\"yo\"><a href=\"#\">link</a><div>foo<br><br>bar<br> <br><br><br>abc</div></body>'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
